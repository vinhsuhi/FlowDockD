# @package _global_

# to execute this experiment run:
# python train.py experiment=flowdock_fm

defaults:
  - override /data: combined
  - override /model: flowdock_fm
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

model:
  optimizer:
    lr: 5e-5
  compile: false
  pretrain_path: /mnt/beegfs/home/ac141281/FlowDockD/checkpoints/esmfold_prior_paper_weights.ckpt

tags: ["flowdock_fm_finetune", "combined_dataset"]

seed: 496

trainer:
  max_epochs: 300
  check_val_every_n_epoch: 5 # NOTE: we increase this since validation steps involve full model sampling and evaluation
  reload_dataloaders_every_n_epochs: 1

data:
  batch_size: 24


logger:
  wandb:
    name: "FlowDock-FM-Finetune"
    tags: ${tags}
    group: "FlowDock-FM"
    save_dir: "${paths.output_dir}"
    offline: False
    id: "suhi_bl01_finetune" # pass correct id to resume experiment!
    anonymous: null # enable anonymous logging
    project: "FlowDock_FM"
    log_model: False # upload lightning ckpts
    prefix: "" # a string to put at the beginning of metric keys
    entity: "yun-ye-lukas-university-of-stuttgart" # set to name of your wandb team
    job_type: "training"

